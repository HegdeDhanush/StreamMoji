# EmoStream: Real-Time Emoji Analytics Platform

## ğŸ“Œ Overview

EmoStream is a real-time event-driven system designed to capture, process, and analyze emoji reactions during live sporting events. Built using Python, Flask, Apache Kafka, and Apache Spark Streaming, the platform demonstrates scalable real-time data ingestion, processing, and distribution architecture.

## ğŸ”§ Tech Stack

* **API Layer**: Flask (REST API)
* **Messaging Backbone**: Apache Kafka
* **Streaming Engine**: Apache Spark Streaming
* **Simulation**: Python scripts (100+ concurrent clients)
* **Monitoring**: Spark metrics tracking
* **Testing**: PyTest

---

## ğŸ§± Architecture

```
[ Clients ]
    | POST /emoji
[ Flask API (main.py, routes.py) ]
    | â†’ Kafka Producer
[ Kafka Topics: main, clusters, output ]
    | â†’ Pub/Sub (main_pubsub.py, pubsub/)
    | â†’ Spark Streaming (processor.py)
    | â†’ Subscribers
```

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ main.py                # Starts Flask API server
â”œâ”€â”€ routes.py              # API routes
â”œâ”€â”€ producer.py            # Kafka producer logic
â”œâ”€â”€ consumer.py            # Kafka consumer logic
â”œâ”€â”€ config.py              # Centralized configs
â”œâ”€â”€ main_pubsub.py         # Starts pub-sub routing logic
â”œâ”€â”€ src/
â”‚   â””â”€â”€ spark/
â”‚       â””â”€â”€ processor.py   # Spark Streaming processor
â”œâ”€â”€ test_clients.py        # Simulates emoji event traffic
â”œâ”€â”€ spark_monitor.py       # Monitors Spark metrics
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ cons_test.py       # Unit tests for consumers
```

---

## ğŸš€ Features

* Real-time emoji ingestion through REST API
* Kafka-based message distribution backbone
* Clustered pub-sub distribution logic
* Spark Streaming windowed aggregation (emoji counts)
* Simulated 100 concurrent clients
* Centralized config and component modularity
* Spark monitoring and system observability
* Unit tested components

---

## ğŸ§ª How to Run

### 1. Start Flask API

```bash
python3 main.py
```

### 2. Start Pub/Sub System

```bash
python3 main_pubsub.py
```

### 3. Start Spark Streaming Job

```bash
spark-submit --master local[*] src/spark/processor.py
```

### 4. Simulate Clients

```bash
python3 test_clients.py --count 100 --duration 60
```

---

## ğŸ“Š Analytics Output (Example)

```json
{
  "window": "2025-08-08T15:00:00Z",
  "counts": {
    "ğŸ˜€": 42,
    "ğŸ”¥": 18,
    "ğŸ˜¡": 5
  }
}
```

---

## âš™ï¸ Configurable Parameters

Edit `config.py` to adjust:

* Kafka broker details
* Topic names
* Spark window durations
* Scaling parameters (clusters, clients)

---

## ğŸ“ˆ Monitoring

* Spark UI at `localhost:4040`
* Metrics include: processing time, throughput, lag
* `spark_monitor.py` logs key statistics

---

## âœ… Testing

```bash
pytest tests/
```

---

## ğŸ“Œ Future Enhancements

* WebSocket client visualization dashboard
* Emoji sentiment analysis
* Cluster-based filtering and personalization
* AI-driven anomaly detection
* Horizontal scaling with Docker/K8s

---

## ğŸ§‘â€ğŸ’» Author

Built by Akash, Computer Science undergrad @ PES University (2022â€“2026).
